[build-system]
requires = ["uv_build>=0.9.6,<0.10.0"]
build-backend = "uv_build"

[tool.uv.build-backend]
module-name = "llama_benchmark"
module-root = ""

[project]
name = "torchao-float8"
version = "0.1.0"
description = "Reproducing the slowdown seen in torchao FP8 weights only config for llama3.1-8B inference."
readme = "README.md"
requires-python = ">=3.12,<3.13"
authors = [
    { name = "Vipul Sharma", email = "vipuls181999@gmail.com" }
]
dependencies = [
    "tokenizers",
    "numpy",
]

[dependency-groups]
slow = [
    "torch==2.8.0",
    "torchao==0.13.0",
]
fast = [
    "torch==2.9.0",
    "torchao==0.14.1",
]

[tool.uv]
python-preference = "only-managed"
default-groups = ["slow"]
conflicts = [
    [
      { group = "slow" },
      { group = "fast" },
    ],
]

[tool.uv.sources]
torch = { index = "pytorch-cu128" }
torchao = { index = "pytorch-cu128" }

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true