# RTX5090 GPU with 34GB VRAM for Llama-3.1-8B's float8wo config peak memory requirements:
# llama-3.1-8B
20251108230606, tok/s=104.68, tok/s_decode=105.73, ttft=0.0186, mem/s=1571.23 GB/s, peak_mem=16.30 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path /root/checkpoints/unsloth/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 
20251108230802, tok/s=160.55, tok/s_decode=169.80, ttft=0.0675, mem/s=1204.97 GB/s, peak_mem= 9.21 GB, model_size= 7.51 GB quant: float8dq-tensor, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq-tensor --checkpoint_path /root/checkpoints/unsloth/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 
20251108231236, tok/s=  7.44, tok/s_decode=  7.48, ttft=0.1442, mem/s=  55.91 GB/s, peak_mem=26.00 GB, model_size= 7.51 GB quant: float8wo, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8wo --checkpoint_path /root/checkpoints/unsloth/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 

# torch memory and execution profile for llama-3.1-8B (baseline, float8dq, float8wo)
20251109194038, tok/s=102.89, tok/s_decode=104.09, ttft=0.0219, mem/s=1544.41 GB/s, peak_mem=16.30 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path /root/checkpoints/unsloth/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --memory_profile torch_memory_profiler --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 
20251109194139, tok/s= 96.47, tok/s_decode=104.18, ttft=0.0208, mem/s=1447.98 GB/s, peak_mem=16.30 GB, model_size=15.01 GB quant: None, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path /root/checkpoints/unsloth/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --profile torch_execution_profiler --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 

20251109194604, tok/s=152.86, tok/s_decode=163.43, ttft=0.0841, mem/s=1147.23 GB/s, peak_mem= 9.21 GB, model_size= 7.51 GB quant: float8dq-tensor, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq-tensor --checkpoint_path /root/checkpoints/unsloth/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --memory_profile torch_memory_profiler --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 
20251109194643, tok/s=138.69, tok/s_decode=162.65, ttft=0.0729, mem/s=1040.86 GB/s, peak_mem= 9.21 GB, model_size= 7.51 GB quant: float8dq-tensor, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq-tensor --checkpoint_path /root/checkpoints/unsloth/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --profile torch_execution_profiler --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 

20251108235126, tok/s=  7.44, tok/s_decode=  7.49, ttft=0.1792, mem/s=  55.86 GB/s, peak_mem=26.00 GB, model_size= 7.51 GB quant: float8wo, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8wo --checkpoint_path /root/checkpoints/unsloth/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --memory_profile torch_memory_profiler --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 
20251108235452, tok/s=  7.24, tok/s_decode=  7.48, ttft=0.1443, mem/s=  54.39 GB/s, peak_mem=26.00 GB, model_size= 7.51 GB quant: float8wo, sparse: None, mod: Meta-Llama-3.1-8B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8wo --checkpoint_path /root/checkpoints/unsloth/Meta-Llama-3.1-8B/model.pth --device cuda --precision torch.bfloat16 --compile --profile torch_execution_profiler --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 

# llama-3.2-3B
20251109191341, tok/s=222.70, tok/s_decode=226.94, ttft=0.0165, mem/s=1430.96 GB/s, peak_mem= 7.45 GB, model_size= 6.43 GB quant: None, sparse: None, mod: Llama-3.2-3B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path /root/checkpoints/unsloth/Llama-3.2-3B/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 
20251109192200, tok/s=269.97, tok/s_decode=294.00, ttft=0.0603, mem/s= 867.40 GB/s, peak_mem= 5.28 GB, model_size= 3.21 GB quant: float8dq-tensor, sparse: None, mod: Llama-3.2-3B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq-tensor --checkpoint_path /root/checkpoints/unsloth/Llama-3.2-3B/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 
20251109191805, tok/s= 19.49, tok/s_decode= 19.61, ttft=0.0601, mem/s=  62.69 GB/s, peak_mem=17.01 GB, model_size= 3.22 GB quant: float8wo, sparse: None, mod: Llama-3.2-3B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8wo --checkpoint_path /root/checkpoints/unsloth/Llama-3.2-3B/model.pth --device cuda --precision torch.bfloat16 --compile --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 

# torch memory and execution profile for llama-3.2-3B (baseline, float8dq, float8wo)
20251109193430, tok/s=221.21, tok/s_decode=226.03, ttft=0.0188, mem/s=1421.37 GB/s, peak_mem= 7.45 GB, model_size= 6.43 GB quant: None, sparse: None, mod: Llama-3.2-3B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path /root/checkpoints/unsloth/Llama-3.2-3B/model.pth --device cuda --precision torch.bfloat16 --compile --memory_profile torch_memory_profiler --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 
20251109193602, tok/s=200.91, tok/s_decode=226.13, ttft=0.0184, mem/s=1290.93 GB/s, peak_mem= 7.45 GB, model_size= 6.43 GB quant: None, sparse: None, mod: Llama-3.2-3B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --checkpoint_path /root/checkpoints/unsloth/Llama-3.2-3B/model.pth --device cuda --precision torch.bfloat16 --compile --profile torch_execution_profiler --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 

20251109193122, tok/s=265.75, tok/s_decode=292.44, ttft=0.0684, mem/s= 853.84 GB/s, peak_mem= 5.28 GB, model_size= 3.21 GB quant: float8dq-tensor, sparse: None, mod: Llama-3.2-3B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq-tensor --checkpoint_path /root/checkpoints/unsloth/Llama-3.2-3B/model.pth --device cuda --precision torch.bfloat16 --compile --memory_profile torch_memory_profiler --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 
20251109193256, tok/s=236.25, tok/s_decode=291.06, ttft=0.0634, mem/s= 759.07 GB/s, peak_mem= 5.28 GB, model_size= 3.21 GB quant: float8dq-tensor, sparse: None, mod: Llama-3.2-3B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8dq-tensor --checkpoint_path /root/checkpoints/unsloth/Llama-3.2-3B/model.pth --device cuda --precision torch.bfloat16 --compile --profile torch_execution_profiler --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 

20251109192510, tok/s= 19.61, tok/s_decode= 19.72, ttft=0.0609, mem/s=  63.06 GB/s, peak_mem=17.01 GB, model_size= 3.22 GB quant: float8wo, sparse: None, mod: Llama-3.2-3B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8wo --checkpoint_path /root/checkpoints/unsloth/Llama-3.2-3B/model.pth --device cuda --precision torch.bfloat16 --compile --memory_profile torch_memory_profiler --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 
20251109192654, tok/s= 18.41, tok/s_decode= 19.54, ttft=0.0604, mem/s=  59.21 GB/s, peak_mem=17.01 GB, model_size= 3.22 GB quant: float8wo, sparse: None, mod: Llama-3.2-3B, kv_quant: False, compile: True, compile_prefill: False, dtype: torch.bfloat16, device: cuda repro: python generate.py --quantization float8wo --checkpoint_path /root/checkpoints/unsloth/Llama-3.2-3B/model.pth --device cuda --precision torch.bfloat16 --compile --profile torch_execution_profiler --num_samples 5 --max_new_tokens 200 --batch_size 1 --top_k 200 --temperature 0.8 

# RTX4090 for only Llama-3.2-3B